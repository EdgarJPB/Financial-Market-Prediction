{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca6c5903",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from pandas_datareader import data as pdr\n",
    "from scipy.special import expit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd7b9fd",
   "metadata": {},
   "source": [
    "### Goal:\n",
    "\n",
    "The goal of this project is to predict the next-day direction (up/down) of the S&P 500 index using both:\n",
    "\n",
    "+ A manually implemented feedforward neural network (FNN)\n",
    "\n",
    "+ A PyTorch-based Long Short-Term Memory (LSTM) model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c48d0b",
   "metadata": {},
   "source": [
    "### Data:\n",
    "\n",
    "+ Market Index: S&P 500 (^GSPC) via Yahoo Finance.\n",
    "\n",
    "+ Volatility: CBOE Volatility Index (^VIX)\n",
    "\n",
    "+ Macroeconomic Indicators from FRED:\n",
    "\n",
    "    + Consumer Price Index (CPI)\n",
    "\n",
    "    + Industrial Production (INDPRO)\n",
    "\n",
    "    + 10-Year Treasury Yield (GS10)\n",
    "\n",
    "    + University of Michigan Consumer Sentiment (UMCSENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10c017c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  5 of 5 completed\n"
     ]
    }
   ],
   "source": [
    "# 1. Download and prepare data\n",
    "#symbols = [\"^GSPC\", \"^DJI\", \"^DJU\", \"^IXIC\", \"^RUT\"]\n",
    "symbols = [\"^GSPC\"]\n",
    "data = yf.download(symbols, start=\"2000-01-01\", end=\"2024-01-01\",auto_adjust=True)[\"Close\"]\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d927c13d",
   "metadata": {},
   "source": [
    "### Feature Engineering:\n",
    "\n",
    "+ For each index:\n",
    "\n",
    "    + Returns: 1-day, 10-days, 30-days,\n",
    "\n",
    "    + Moving Averages: 10-day, 30-day\n",
    "\n",
    "    + Volatility: 10-day  and 30-days rolling std dev\n",
    "\n",
    "    + Momentum: price differences over 10 and 30 days\n",
    "\n",
    "+ Technical Indicators:\n",
    "\n",
    "    + RSI (Relative Strength Index)\n",
    "\n",
    "    + MACD (Moving Average Convergence Divergence)\n",
    "\n",
    "    + Bollinger Bands + Bollinger percentage\n",
    "\n",
    "+ Macroeconomic & Volatility Features:\n",
    "\n",
    "    + Log returns of CPI, Industrial Production, etc.\n",
    "\n",
    "    + Daily % change in VIX\n",
    "\n",
    "The final feature matrix includes over 50 features per day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db353665",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edgar\\AppData\\Local\\Temp\\ipykernel_20756\\4054702977.py:45: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  features[[f'{k}_chg' for k in fred_symbols]] = features[[f'{k}_chg' for k in fred_symbols]].fillna(method='ffill')\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "# 2. Feature Engineering\n",
    "features = pd.DataFrame(index=data.index)\n",
    "for col in data.columns:\n",
    "    features[f'{col}_return_1d'] = data[col].pct_change()\n",
    "    # features[f'{col}_return_5d'] = data[col].pct_change(5)\n",
    "    features[f'{col}_return_10d'] = data[col].pct_change(10)\n",
    "    features[f'{col}_return_30d'] = data[col].pct_change(30)\n",
    "    #features[f'{col}_ma_5'] = data[col].rolling(5).mean()\n",
    "    features[f'{col}_ma_10'] = data[col].rolling(10).mean()\n",
    "    features[f'{col}_ma_30'] = data[col].rolling(30).mean()\n",
    "    #features[f'{col}_vol_5'] = data[col].rolling(5).std()\n",
    "    features[f'{col}_vol_10'] = data[col].rolling(10).std()\n",
    "    features[f'{col}_vol_30'] = data[col].rolling(30).std()\n",
    "    #features[f'{col}_momentum_5d'] = data[col] - data[col].shift(5)\n",
    "    features[f'{col}_momentum_10d'] = data[col] - data[col].shift(10)\n",
    "    features[f'{col}_momentum_30d'] = data[col] - data[col].shift(30)\n",
    "\n",
    "# Technical Indicators \n",
    "sp500 = data[\"^GSPC\"]\n",
    "# RSI\n",
    "delta = sp500.diff()\n",
    "gain = delta.clip(lower=0).rolling(14).mean()\n",
    "loss = -delta.clip(upper=0).rolling(14).mean()\n",
    "rs = gain / loss\n",
    "features['rsi'] = 100 - (100 / (1 + rs))\n",
    "# MACD\n",
    "ema12 = sp500.ewm(span=12, adjust=False).mean()\n",
    "ema26 = sp500.ewm(span=26, adjust=False).mean()\n",
    "features['macd'] = ema12 - ema26\n",
    "# Bollinger Bands\n",
    "rolling_mean = sp500.rolling(20).mean()\n",
    "rolling_std = sp500.rolling(20).std()\n",
    "features['boll_upper'] = rolling_mean + 2 * rolling_std\n",
    "features['boll_lower'] = rolling_mean - 2 * rolling_std\n",
    "features['boll_pct'] = (sp500 - features['boll_lower']) / (features['boll_upper'] - features['boll_lower'])\n",
    "\n",
    "fred_symbols = {'cpi': 'CPIAUCSL', 'ind_prod': 'INDPRO', '10yr_treasury': 'GS10','consumer_sentiment': 'UMCSENT'}\n",
    "for name, code in fred_symbols.items():\n",
    "    fred_data = pdr.DataReader(code, 'fred', start=\"2000-01-01\", end=\"2024-01-01\")\n",
    "    fred_data = np.log(fred_data).diff()  # log returns\n",
    "    fred_data = fred_data.rename(columns={code: f'{name}_chg'})\n",
    "    features = features.join(fred_data, how='left')\n",
    "\n",
    "# Forward-fill to match daily frequency\n",
    "features[[f'{k}_chg' for k in fred_symbols]] = features[[f'{k}_chg' for k in fred_symbols]].fillna(method='ffill')\n",
    "\n",
    "# Add VIX (market volatility)\n",
    "vix = yf.download(\"^VIX\", start=\"2000-01-01\", end=\"2024-01-01\",auto_adjust=True)[\"Close\"].pct_change()\n",
    "vix.name = \"vix_return\"\n",
    "features = features.join(vix, how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff27881",
   "metadata": {},
   "source": [
    "### Target Variable\n",
    "+ Binary classification:\n",
    "\n",
    "    + 1: Next-day return of S&P 500 > 0\n",
    "\n",
    "    + 0: Otherwise\n",
    "\n",
    "### Preprocessing\n",
    "+ NaN handling: Forward fill for macro data; full row drop where necessary\n",
    "\n",
    "+ Standardization: All features scaled using StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96ae0364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Target variable\n",
    "# Drop rows with NaNs before creating the target\n",
    "features = features.dropna()\n",
    "\n",
    "# Create target AFTER all features are complete\n",
    "threshold = 0.01\n",
    "future_return = data[\"^GSPC\"].pct_change(5).shift(-5)\n",
    "future_return = future_return.reindex(features.index)  # align with features\n",
    "\n",
    "# Define target only where both future return and features are available\n",
    "features['target'] = np.where(future_return > threshold, 1,\n",
    "                       np.where(future_return < -threshold, 0, np.nan))\n",
    "features = features.dropna(subset=['target'])  # drop unlabeled rows\n",
    "\n",
    "# 4. Prepare data\n",
    "assert not features.drop(columns='target').isnull().any().any(), \"There are still NaNs in the features\"\n",
    "X = features.drop(columns='target').values\n",
    "y = features['target'].values\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 5. Walk-forward evaluation\n",
    "split_size = int(len(X_scaled) * 0.8)\n",
    "X_train, X_test = X_scaled[:split_size], X_scaled[split_size:]\n",
    "y_train, y_test = y[:split_size], y[split_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0b7d5c",
   "metadata": {},
   "source": [
    "### Models:\n",
    "+ Manual Feedforward Neural Network (FNN)\n",
    "    \n",
    "    + Implemented from scratch\n",
    "\n",
    "    + Architecture: 10 hidden layers with [4096, 2048, 1024, 512, 256, 128, 64, 32, 16, 8] neurons\n",
    "\n",
    "    + Activation: ReLU + Sigmoid (output)\n",
    "\n",
    "    + Regularization: Dropout (0.2), L2 regularization\n",
    "\n",
    "    + Optimizer: Adam (manual implementation)\n",
    "\n",
    "    + Early Stopping based on validation loss\n",
    "\n",
    "+ PyTorch LSTM Model\n",
    "\n",
    "    + LSTM layers with 256 hidden units and 4 layers\n",
    "\n",
    "    + Dense + Sigmoid output layer\n",
    "\n",
    "    + Dropout (0.3) between layers\n",
    "\n",
    "    + Class imbalance handled using pos_weight in BCEWithLogitsLoss\n",
    "\n",
    "    + Optimized with Adam\n",
    "\n",
    "    + Trained using mini-batch time series windows ( 30-day lookback)\n",
    "\n",
    "    + Includes early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84bc45c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Manual Neural Network (unchanged from previous step)\n",
    "class ManualNeuralNet:\n",
    "    def __init__(self, input_dim, layer_sizes, output_dim=1, dropout_rate=0.2, lambda_reg=0.01):\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self.activations = []\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.t = 0\n",
    "        self.m, self.v = {}, {}\n",
    "\n",
    "        layer_dims = [input_dim] + layer_sizes + [output_dim]\n",
    "        for i in range(len(layer_dims) - 1):\n",
    "            W = np.random.randn(layer_dims[i], layer_dims[i+1]) * 0.01\n",
    "            b = np.zeros((1, layer_dims[i+1]))\n",
    "            self.weights.append(W)\n",
    "            self.biases.append(b)\n",
    "            self.m[f\"W{i}\"] = np.zeros_like(W)\n",
    "            self.m[f\"b{i}\"] = np.zeros_like(b)\n",
    "            self.v[f\"W{i}\"] = np.zeros_like(W)\n",
    "            self.v[f\"b{i}\"] = np.zeros_like(b)\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        #return 1 / (1 + np.exp(-z))\n",
    "        return expit(z) \n",
    "    \n",
    "    def relu(self, z):\n",
    "        return np.maximum(0, z)\n",
    "\n",
    "    def relu_derivative(self, z):\n",
    "        return (z > 0).astype(float)\n",
    "\n",
    "    def forward(self, X, training=True):\n",
    "        self.Zs, self.As = [], [X]\n",
    "        for i in range(len(self.weights) - 1):\n",
    "            Z = self.As[-1] @ self.weights[i] + self.biases[i]\n",
    "            A = self.relu(Z)\n",
    "            if training:\n",
    "                A *= np.random.binomial(1, 1 - self.dropout_rate, size=A.shape)\n",
    "            self.Zs.append(Z)\n",
    "            self.As.append(A)\n",
    "        Z_final = self.As[-1] @ self.weights[-1] + self.biases[-1]\n",
    "        A_final = self.sigmoid(Z_final)\n",
    "        self.Zs.append(Z_final)\n",
    "        self.As.append(A_final)\n",
    "        return A_final\n",
    "\n",
    "    def compute_loss(self, Y_hat, Y):\n",
    "        m = Y.shape[0]\n",
    "        reg = self.lambda_reg * sum(np.sum(np.square(w)) for w in self.weights) / (2 * m)\n",
    "        return -np.sum(Y * np.log(Y_hat + 1e-8) + (1 - Y) * np.log(1 - Y_hat + 1e-8)) / m + reg\n",
    "\n",
    "    def backward(self, X, Y, lr=0.01, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        m = Y.shape[0]\n",
    "        self.t += 1\n",
    "        grads_W, grads_b = [], []\n",
    "\n",
    "        dZ = self.As[-1] - Y.reshape(-1, 1)\n",
    "        for i in reversed(range(len(self.weights))):\n",
    "            dW = self.As[i].T @ dZ / m + self.lambda_reg * self.weights[i] / m\n",
    "            db = np.sum(dZ, axis=0, keepdims=True) / m\n",
    "            grads_W.insert(0, dW)\n",
    "            grads_b.insert(0, db)\n",
    "\n",
    "            if i > 0:\n",
    "                dA = dZ @ self.weights[i].T\n",
    "                dZ = dA * self.relu_derivative(self.Zs[i-1])\n",
    "\n",
    "        for i in range(len(self.weights)):\n",
    "            self.m[f\"W{i}\"] = beta1 * self.m[f\"W{i}\"] + (1 - beta1) * grads_W[i]\n",
    "            self.v[f\"W{i}\"] = beta2 * self.v[f\"W{i}\"] + (1 - beta2) * (grads_W[i] ** 2)\n",
    "            self.m[f\"b{i}\"] = beta1 * self.m[f\"b{i}\"] + (1 - beta1) * grads_b[i]\n",
    "            self.v[f\"b{i}\"] = beta2 * self.v[f\"b{i}\"] + (1 - beta2) * (grads_b[i] ** 2)\n",
    "            m_hat_W = self.m[f\"W{i}\"] / (1 - beta1 ** self.t)\n",
    "            v_hat_W = self.v[f\"W{i}\"] / (1 - beta2 ** self.t)\n",
    "            m_hat_b = self.m[f\"b{i}\"] / (1 - beta1 ** self.t)\n",
    "            v_hat_b = self.v[f\"b{i}\"] / (1 - beta2 ** self.t)\n",
    "            self.weights[i] -= lr * m_hat_W / (np.sqrt(v_hat_W) + epsilon)\n",
    "            self.biases[i] -= lr * m_hat_b / (np.sqrt(v_hat_b) + epsilon)\n",
    "\n",
    "    def train(self, X, Y, epochs=200, lr=0.001, early_stopping_rounds=5    ):\n",
    "        self.losses = []\n",
    "        best_loss = float('inf')\n",
    "        rounds_no_improve = 0\n",
    "        for epoch in range(epochs):\n",
    "            Y_hat = self.forward(X)\n",
    "            loss = self.compute_loss(Y_hat, Y)\n",
    "            self.losses.append(loss)\n",
    "            self.backward(X, Y, lr=lr)\n",
    "            if loss < best_loss:\n",
    "                best_loss = loss\n",
    "                rounds_no_improve = 0\n",
    "            else:\n",
    "                rounds_no_improve += 1\n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "            if rounds_no_improve >= early_stopping_rounds:\n",
    "                print(f\"Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "\n",
    "    def predict(self, X, threshold=0.5):\n",
    "        return (self.forward(X, training=False) > threshold).astype(int)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "789f79af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1974.0851\n",
      "Epoch 10, Loss: 1972.2591\n",
      "Epoch 20, Loss: 1970.4058\n",
      "Epoch 30, Loss: 1968.4792\n",
      "Epoch 40, Loss: 1966.3775\n",
      "Epoch 50, Loss: 1963.8743\n",
      "Epoch 60, Loss: 1960.1602\n",
      "Epoch 70, Loss: 1955.3205\n",
      "Early stopping at epoch 73\n",
      "Manual NN Test Accuracy: 62.27 %\n"
     ]
    }
   ],
   "source": [
    "# Train manual network\n",
    "manual_nn = ManualNeuralNet(input_dim=X_train.shape[1], layer_sizes=[4096, 2048, 1024, 512, 256, 128, 64, 32, 16, 8], output_dim=1)\n",
    "manual_nn.train(X_train, y_train, epochs=200, lr=0.001)\n",
    "\n",
    "# ROC and evaluation for manual net\n",
    "y_probs = manual_nn.forward(X_test, training=False).flatten()\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_probs)\n",
    "optimal_idx = np.argmax(tpr - fpr)\n",
    "optimal_threshold = thresholds[optimal_idx]\n",
    "y_pred = (y_probs > optimal_threshold).astype(int)\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print(\"Manual NN Test Accuracy:\", round(accuracy * 100, 2), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27f3a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best LSTM Accuracy: 54.46 %\n",
      "Best Params: {'dropout': 0.3, 'hidden_dim': 256, 'lr': 0.001, 'num_layers': 4}\n",
      "Manual NN Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.54      0.08      0.14       272\n",
      "         1.0       0.63      0.96      0.76       441\n",
      "\n",
      "    accuracy                           0.62       713\n",
      "   macro avg       0.58      0.52      0.45       713\n",
      "weighted avg       0.59      0.62      0.52       713\n",
      "\n",
      "LSTM Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.43      0.56      0.48       269\n",
      "         1.0       0.66      0.53      0.59       438\n",
      "\n",
      "    accuracy                           0.54       707\n",
      "   macro avg       0.55      0.55      0.54       707\n",
      "weighted avg       0.57      0.54      0.55       707\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 7. PyTorch LSTM\n",
    "class LSTMNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=256, num_layers=4, dropout=0.3, bidirectional=True):\n",
    "        super(LSTMNet, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout, bidirectional=bidirectional)\n",
    "        direction_factor = 2 if bidirectional else 1\n",
    "        self.fc = nn.Linear(hidden_dim * direction_factor, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = out[:, -1, :]\n",
    "        return self.fc(out)\n",
    "    \n",
    "# Prepare LSTM sequences\n",
    "time_steps = 30\n",
    "X_lstm, y_lstm = [], []\n",
    "for i in range(time_steps, len(X_scaled)):\n",
    "    X_lstm.append(X_scaled[i - time_steps:i])\n",
    "    y_lstm.append(y[i])\n",
    "X_lstm = np.array(X_lstm)\n",
    "y_lstm = np.array(y_lstm)\n",
    "\n",
    "split_idx = int(len(X_lstm) * 0.8)\n",
    "X_lstm_train, X_lstm_test = X_lstm[:split_idx], X_lstm[split_idx:]\n",
    "y_lstm_train, y_lstm_test = y_lstm[:split_idx], y_lstm[split_idx:]\n",
    "\n",
    "X_lstm_train_t = torch.tensor(X_lstm_train, dtype=torch.float32)\n",
    "y_lstm_train_t = torch.tensor(y_lstm_train.reshape(-1, 1), dtype=torch.float32)\n",
    "X_lstm_test_t = torch.tensor(X_lstm_test, dtype=torch.float32)\n",
    "y_lstm_test_t = torch.tensor(y_lstm_test.reshape(-1, 1), dtype=torch.float32)\n",
    "\n",
    "# Compute class weights\n",
    "pos_weight_value = (y_lstm_train == 0).sum() / (y_lstm_train == 1).sum()\n",
    "pos_weight = torch.tensor([pos_weight_value], dtype=torch.float32)\n",
    "\n",
    "# Hyperparameter grid\n",
    "#hyperparams_grid = {'hidden_dim': [128, 256], 'num_layers': [2,4], 'dropout': [0.2,0.3], 'lr': [0.0005, 0.001]}\n",
    "hyperparams_grid = {'hidden_dim': [256], 'num_layers': [4], 'dropout': [0.3], 'lr': [0.001]}\n",
    "\n",
    "best_acc = 0\n",
    "best_model = None\n",
    "best_params = None\n",
    "\n",
    "for hp in ParameterGrid(hyperparams_grid):\n",
    "    model = LSTMNet(\n",
    "        input_dim=X_lstm.shape[2],\n",
    "        hidden_dim=hp['hidden_dim'],\n",
    "        num_layers=hp['num_layers'],\n",
    "        dropout=hp['dropout'],\n",
    "        bidirectional=True\n",
    "    )\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=hp['lr'])\n",
    "\n",
    "    best_loss = float('inf')\n",
    "    no_improve_count = 0\n",
    "\n",
    "    for epoch in range(50):\n",
    "        model.train()\n",
    "        output = model(X_lstm_train_t)\n",
    "        loss = criterion(output, y_lstm_train_t)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if loss.item() < best_loss:\n",
    "            best_loss = loss.item()\n",
    "            no_improve_count = 0\n",
    "        else:\n",
    "            no_improve_count += 1\n",
    "\n",
    "        if no_improve_count >= 5:\n",
    "            break\n",
    "\n",
    "    model.eval()\n",
    "    y_logits = model(X_lstm_test_t).detach().numpy().flatten()\n",
    "    y_probs = 1 / (1 + np.exp(-y_logits))\n",
    "    y_preds = (y_probs > 0.5).astype(int)\n",
    "    acc = np.mean(y_preds == y_lstm_test)\n",
    "\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_model = model\n",
    "        best_params = hp\n",
    "\n",
    "print(\"Best LSTM Accuracy:\", round(best_acc * 100, 2), \"%\")\n",
    "print(\"Best Params:\", best_params)\n",
    "\n",
    "best_model.eval()\n",
    "y_logits = best_model(X_lstm_test_t).detach().numpy().flatten()\n",
    "y_probs = 1 / (1 + np.exp(-y_logits))\n",
    "y_lstm_pred = (y_probs > 0.5).astype(int)\n",
    "\n",
    "# Classification report\n",
    "print(\"Manual NN Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"LSTM Report:\")\n",
    "print(classification_report(y_lstm_test, y_lstm_pred))\n",
    "#\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438706a0",
   "metadata": {},
   "source": [
    "+ The manual NN overfits to the dominant class (1) due to class imbalance.\n",
    "\n",
    "+ The LSTM shows more balanced precision and recall but underperforms in overall accuracy.\n",
    "\n",
    "+ Threshold tuning, feature selection, or switching to a multi-class problem may offer improvement."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "primary",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
